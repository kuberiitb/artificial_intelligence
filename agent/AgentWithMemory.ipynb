{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYFwuZDVv3xNR/GBekXXmA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuberiitb/artificial_intelligence/blob/main/agent/AgentWithMemory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IDmDSMEWbX8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup packages"
      ],
      "metadata": {
        "id": "VOeaCxR6XAqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain-openai langgraph --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJBLhGdAXDB-",
        "outputId": "2158cafb-03a8-47ba-b61f-791708656904"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\".env\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTmZi1V8XDdi",
        "outputId": "83754d77-fa4c-4963-e0b2-c106d3fe58cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "if not os.environ[\"OPENAI_API_KEY\"]:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIn0EPMBXKvC",
        "outputId": "cab4b805-92ad-4d3e-89c0-fb4413bf26f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4154842141.py:8: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"Hi\").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jPvSO8ulXNYp",
        "outputId": "04f2425f-bae9-4466-9896-19fc2cb4e030"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Built using tutorial [build basic chatbot](https://langchain-ai.github.io/langgraph/tutorials/get-started/1-build-basic-chatbot/#3-add-a-node) and [Add Memory](https://langchain-ai.github.io/langgraph/tutorials/get-started/3-add-memory/#4-ask-a-follow-up-question)"
      ],
      "metadata": {
        "id": "xEnHR7TvX7f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)"
      ],
      "metadata": {
        "id": "pNe2BDrPXeQe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are a helpful assistant that answers user's query in short and simple language.\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "chain = prompt | llm\n",
        "\n",
        "def chatbot(state: State):\n",
        "  response = chain.invoke(\n",
        "        {\n",
        "            \"input\": state[\"messages\"],\n",
        "        }\n",
        "    )\n",
        "  return {\"messages\": [response.content]}\n",
        "\n",
        "\n",
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# the node is used.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "norVPNZAX0vZ",
        "outputId": "99870039-1bee-4797-9ade-bb977b10691e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x79b93dcadf40>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add an entry point to tell the graph where to start its work each time it is run:\n",
        "graph_builder.add_edge(START, \"chatbot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddrg8mSAX4Mk",
        "outputId": "a8f38ebe-f22a-4038-c0a4-c2f96dc10768"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x79b93dcadf40>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add an exit point to indicate where the graph should finish execution.\n",
        "graph_builder.add_edge(\"chatbot\", END)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKTLGNqBYPVk",
        "outputId": "e819e2a9-024f-446c-f974-8d07f4a5be55"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x79b93dcadf40>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "UbadgTZgYX1C"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "8f2YgKYEYYKo",
        "outputId": "a8540f9a-252a-4c09-d7b6-860ed032515e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1])"
      ],
      "metadata": {
        "id": "f-GTORDIYbab"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the graph in loop\n",
        "questions = [\"Who is India's prime minister?\",\"What is his birthdate?\",\"What about his mother's name?\", \"exit\"]\n",
        "\n",
        "idx = 0\n",
        "while True:\n",
        "    try:\n",
        "        user_input = questions[idx] #input(\"User: \")\n",
        "        idx+=1\n",
        "        print(\"User: \" + user_input)\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCKZb7wWYpVM",
        "outputId": "bbfa3d83-9d09-4718-a069-c06a977422d4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Who is India's prime minister?\n",
            "Assistant: India's Prime Minister is Narendra Modi.\n",
            "User: What is his birthdate?\n",
            "Assistant: I need more context to answer that. Whose birthdate are you asking about?\n",
            "User: What about his mother's name?\n",
            "Assistant: I'm sorry, but I need more context to answer your question about his mother's name. Could you please provide more details?\n",
            "User: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Short term Memory"
      ],
      "metadata": {
        "id": "Jcnkk4Gcd27d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "memory = InMemorySaver()"
      ],
      "metadata": {
        "id": "-uKKi2O2Yyw0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_with_memory = graph_builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "bMPzQahpd53Y"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### let's see how does it work.\n",
        "#### It uses thread id to map the messages."
      ],
      "metadata": {
        "id": "BLKcKgRwiG9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"Hi there! My name is Kuber.\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph_with_memory.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "     {\"configurable\": {\"thread_id\": \"1\"}},\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xGYO3XFeDpP",
        "outputId": "3f972082-6ef9-4b29-aee5-031044aee90b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi there! My name is Kuber.\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Hi Kuber! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"Tell me what is my name?\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    {\"configurable\": {\"thread_id\": \"1\"}},\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgSwG-sOeF5I",
        "outputId": "ae024d9a-86db-48fb-9437-9f929cd437ee"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Tell me what is my name?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Your name is Kuber! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What happens if we change the thread id?"
      ],
      "metadata": {
        "id": "L-c-BU_KhzPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"Tell me what is my name again?\"\n",
        "\n",
        "# The config is the **second positional argument** to stream() or invoke()!\n",
        "events = graph.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
        "    {\"configurable\": {\"thread_id\": \"2\"}},\n",
        "    stream_mode=\"values\",\n",
        ")\n",
        "for event in events:\n",
        "    event[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YQ4-9AMhvEN",
        "outputId": "dd9c75cc-ec62-475f-aa63-5f8a21a561ad"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Tell me what is my name again?\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "I'm sorry, but I don't know your name. Can you tell me?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Without correct thread it, memory will not work!!"
      ],
      "metadata": {
        "id": "GRSnDvNyijHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's add it to the function"
      ],
      "metadata": {
        "id": "dZaObqJPiqFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph_with_memory.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config):\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1])"
      ],
      "metadata": {
        "id": "95nOU3yaeM04"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the graph in loop\n",
        "questions = [\"Who is India's prime minister?\",\"What is his birthdate?\",\"What about his mother's name?\", \"exit\"]\n",
        "\n",
        "idx = 0\n",
        "while True:\n",
        "    try:\n",
        "        user_input = questions[idx] #input(\"User: \")\n",
        "        idx+=1\n",
        "        print(\"User: \" + user_input)\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        # fallback if input() is not available\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQ_lit1uej1Q",
        "outputId": "81227d13-624d-40c5-f637-9bac90ad503b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Who is India's prime minister?\n",
            "Assistant: India's Prime Minister is Narendra Modi.\n",
            "User: What is his birthdate?\n",
            "Assistant: Narendra Modi was born on September 17, 1950.\n",
            "User: What about his mother's name?\n",
            "Assistant: Narendra Modi's mother's name is Heeraben Modi.\n",
            "User: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Great, now our chatbot has short term memory as well!!"
      ],
      "metadata": {
        "id": "RMnI7tkyjA0N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUAMjOUtjJkR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}