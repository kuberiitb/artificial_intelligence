{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJWQxz0aXoGk"
   },
   "source": [
    "**Let's see how does pytesseract work for text extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "MjHT9_MWGb9f"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain pdf2image pytesseract pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3CyFc1rCHC0e",
    "outputId": "e1084548-59d3-4151-8f9c-0a1728b803d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "poppler-utils is already the newest version (22.02.0-2ubuntu0.10).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# need poppler for pytesseract\n",
    "!apt-get install -y poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b4iKCvMAGbNI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Vq0rA03AGqGC"
   },
   "outputs": [],
   "source": [
    "pdf_path = '/content/Chapter1CinderellaCWnotes.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qq7x3lJCGjIP"
   },
   "outputs": [],
   "source": [
    "def extract_text_from_scanned_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from scanned PDF using OCR.\"\"\"\n",
    "    # Convert PDF pages to images\n",
    "    pages = convert_from_path(pdf_path)\n",
    "\n",
    "    extracted_text = \"\"\n",
    "    for page_number, page in enumerate(pages, start=0):\n",
    "        text = pytesseract.image_to_string(page, lang=\"eng\")\n",
    "        extracted_text += f\"\\n--- Page {page_number} ---\\n{text}\"\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEAdWrJjYBlS"
   },
   "source": [
    "Let's check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "o-4DfKlfG8Pz",
    "outputId": "b9144a7a-cf48-4593-a96c-cbe0da470af7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n--- Page 0 ---\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nI\\nit vor\\n\\n \\n\\x0c\\n--- Page 1 ---\\n     \\n  \\n    \\n  \\n  \\n  \\n \\n \\n\\nZ '\\n\\n  \\n\\nCae\\n“sat\\n\\ni\\n=e rs\\n\\nDATE:\\n\\nae\\n\\n   \\n\\nHA” Mess QLoe bo people:\\n\\n3 |] R oye a ae o tek\\n\\na 4-1 Batt oe rage [0 mal party “wr\\n\\ndancing =\\n\\n=3-{Me :\\n—— 0 ER Ly opie ty Ca\\n\\n \\n\\n \\n\\n \\n\\ni a\\nee aid ap nig tea clocksot ign\\n\\nHF rion—and Ye\\niL eSLION rYYIS¥V AY\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n—t!: ‘Why A tne Tung s Messe! ger\\n\\nEe at -Cinde rer) ASC Avors pep\\n\\nA :\\nA ne Kinz BS Mhesseng er_cam MH ce\\n\\nPer aa\\n\\n—\\n\\n¥.\\n\\x0c\\n--- Page 2 ---\\n \\n\\n \\n\\x0c\\n--- Page 3 ---\\n \\n     \\n   \\n    \\n  \\n \\n  \\n    \\n\\n  \\n\\nD)\\n\\n: sa 6) eae\\ntep MONEY aia NOT\\n\\n~ —Gnderella to attend the bak\\n\\na\\n\\nYoo CNET S =o a\\n\\n \\n\\n \\n\\nN = ee\\nMASCuNe Vr enminine\\nPUN eran ter\\na. a) ane\\n\\nae\\neT TOOL\\n\\n   \\n\\x0c\\n--- Page 4 ---\\ni) | an\\n2a\\n\\n \\n\\neM\\n\\nat\\n\\nTTT IO ee\\npO are\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\x0c\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text_from_scanned_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWKnIIROYE8c"
   },
   "source": [
    "Output from pytesseract is quite bad. Not able to detect even a single word.\n",
    "\n",
    "Let's check the pages quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GyK39Ks5G-UN"
   },
   "outputs": [],
   "source": [
    "pages = convert_from_path(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "-LdxraEsYeiI"
   },
   "outputs": [],
   "source": [
    "# from IPython.display import display\n",
    "\n",
    "# for idx, page in enumerate(pages):\n",
    "#   print(f\"Page number {idx}\")\n",
    "#   display(page)\n",
    "#   break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPBTuWd_Z_c0"
   },
   "source": [
    "Now see how pytesseract is reading page number 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "nflYnIO7HyDm",
    "outputId": "a6fa8452-f1db-4718-cf84-ab525664f425"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nI\\nit vor\\n\\n \\n\\x0c'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytesseract.image_to_string(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5gkSzSNaGMT"
   },
   "source": [
    "**As we see, pytesseract did quite poor job. Instead of optimizing it, let's try few other approaches quickly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPRv8KGfaSo1"
   },
   "source": [
    "Let's try easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "0ChkHwhQH6G_"
   },
   "outputs": [],
   "source": [
    "!pip install -q easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2g9oYBlIJVL",
    "outputId": "8e8fcee8-7d91-41a8-9eba-1df01ad047b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "reader = easyocr.Reader(['en'])  # supports multiple languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5RrXIjlchWq"
   },
   "source": [
    "Let's see how well easyocr is able to read the first page. We need to pass the image from pdf to easyocr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84h3ASqeIToZ",
    "outputId": "11e33a59-ac61-40a4-929d-39126f9b5b8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DATE:',\n",
       " 'PAGE:',\n",
       " 'hatEer]',\n",
       " 'Cin derella',\n",
       " 'TEuzl',\n",
       " 'GormoHeT',\n",
       " 'essenger',\n",
       " 'TmUd',\n",
       " 'IiLalion',\n",
       " '(Iass',\n",
       " 'Slipper',\n",
       " 'OOI',\n",
       " 'Cinderea',\n",
       " 'meLTgE',\n",
       " '€rul',\n",
       " 'S0pORe',\n",
       " 'KeS',\n",
       " 'PAII',\n",
       " 'LO',\n",
       " 'oLers',\n",
       " 'exenger',\n",
       " 'PeESon',\n",
       " 'Who',\n",
       " 'NOLd',\n",
       " 'uf',\n",
       " 'gieS']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].save('/content/Chapter1CinderellaCWnotes.jpeg')\n",
    "result = reader.readtext('/content/Chapter1CinderellaCWnotes.jpeg', detail=0)  # detail=0 returns only text\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcOkih-kc_0b"
   },
   "source": [
    "easyocr is able to read few words, **but mostly incorrectly**.\n",
    "Also adding many junk words like TEuzl, meLTgE etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLZW9vnHcwGb"
   },
   "source": [
    "Now let's try **Transformer Vision Model** for OCR task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "cdcd88effa124a5e9c40c25f8dd336df",
      "aa0beb1c65ce4e17a983b5e59bbeec6e",
      "ad4f2bf792674683bb929c7e82b2a49b",
      "2eea0fdb37214d6bab9fd7fb01ecc42c",
      "e54bad369d04446b808a8d0ae250796e",
      "d8b4045611354038a3245658bff81f34",
      "e40d834c1f1d49988d8cf7dc50625a1e",
      "0496c7078b3a44978b5ffffb5801a5fa",
      "66e5dfa5cef240e5b013883a831d8c39",
      "65a541da27ba477482606b3319bce285",
      "e7698ef59bf04e42baa881c63121432d"
     ]
    },
    "id": "9rFQpkDoI2oY",
    "outputId": "ad207b13-c963-4219-a56e-f403593e3789"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcd88effa124a5e9c40c25f8dd336df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tSr3Z7S9JrN7",
    "outputId": "3a7368b6-75c6-4684-cecc-f52555b227d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Print export']\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('/content/Chapter1CinderellaCWnotes.jpeg')\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values)\n",
    "text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyDpf8j4dljf"
   },
   "source": [
    "Transformer is giving totally unrelated output.\n",
    "\n",
    "Let's try another page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gs6F0ybSdxLa",
    "outputId": "0bad401e-9486-4af8-fda0-6ed12d7b67b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You must be able to provide a close connection to the']\n"
     ]
    }
   ],
   "source": [
    "pages[1].save('/content/Chapter1CinderellaCWnotes1.jpeg')\n",
    "image = Image.open('/content/Chapter1CinderellaCWnotes1.jpeg')\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "generated_ids = model.generate(pixel_values)\n",
    "text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVxWECYYeLwe"
   },
   "source": [
    "Even on this page, the output is totally unrelated.\n",
    "\n",
    "Pure case of Hallucination.\n",
    "\n",
    "Let's verify the image we passed again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "rTGn-kHgJ8Ks"
   },
   "outputs": [],
   "source": [
    "# display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzkWC9epeh8k"
   },
   "source": [
    "Since all opensource approaches failed, let's try how **AWS Textract **work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "nB8Tp_9rLnCd"
   },
   "outputs": [],
   "source": [
    "!pip install -q boto3 dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "ySYYuOBXJ_x6"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('/content/.env')\n",
    "#setting aws configurations\n",
    "access_key_id = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_access_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "region_name = os.getenv(\"AWS_REGION\")\n",
    "\n",
    "client = boto3.client(\"textract\", region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLm3whExe6uz"
   },
   "source": [
    "Now let's pass page 1 to AWS Textract and check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsTdtT4zL2sc",
    "outputId": "3ba90870-34aa-4ebc-8a43-83602c0995bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE:\n",
      "PAGE:\n",
      "Chapter-1\n",
      "Cinderella\n",
      "I\n",
      "Word wall :-\n",
      "Cruel\n",
      "Fairy Godmother\n",
      "Messenger Warned\n",
      "Invitation Glass slipper\n",
      "Royal ball Cinderella\n",
      "11\n",
      "Word meaning:\n",
      "1.\n",
      "Cruel- - someone who causes pain\n",
      "to others\n",
      "2.\n",
      "Messenger - a person who gives\n"
     ]
    }
   ],
   "source": [
    "# Read the file bytes\n",
    "with open('/content/Chapter1CinderellaCWnotes.jpeg', \"rb\") as f:\n",
    "    pdf_bytes = f.read()\n",
    "\n",
    "# Call Textract\n",
    "response = client.analyze_document(\n",
    "    Document={\"Bytes\": pdf_bytes},\n",
    "    FeatureTypes=[\"TABLES\", \"FORMS\"]\n",
    ")\n",
    "# Extract text from blocks\n",
    "text = []\n",
    "for block in response[\"Blocks\"]:\n",
    "    if block[\"BlockType\"] == \"LINE\":\n",
    "      text.append(block[\"Text\"])\n",
    "\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4wFW-7QfJ_8"
   },
   "source": [
    "Quite good. let's try next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnw8kf7aMWqA",
    "outputId": "a4cb1e7d-46c1-4c54-fc1b-b97314c211b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE:\n",
      "PAGE:\n",
      "a message to people\n",
      "3.\n",
      "Royal - related to the king or queen\n",
      "4.\n",
      "Ball - a large formal party with\n",
      "dancing\n",
      "5.\n",
      "Meekly - quietly\n",
      "6.\n",
      "Midnight - 12:0'clock at night\n",
      "III\n",
      "Question and Answer\n",
      "Q1.\n",
      "Why did the King's messenger come\n",
      "at Cinderella's doorstep?\n",
      "A1.\n",
      "The King's messenger came at\n",
      "Cinderella's doorstep with an\n"
     ]
    }
   ],
   "source": [
    "# Read the file bytes\n",
    "with open('/content/Chapter1CinderellaCWnotes1.jpeg', \"rb\") as f:\n",
    "    pdf_bytes = f.read()\n",
    "\n",
    "# Call Textract\n",
    "response = client.analyze_document(\n",
    "    Document={\"Bytes\": pdf_bytes},\n",
    "    FeatureTypes=[\"TABLES\", \"FORMS\"]\n",
    ")\n",
    "# Extract text from blocks\n",
    "text = []\n",
    "for block in response[\"Blocks\"]:\n",
    "    if block[\"BlockType\"] == \"LINE\":\n",
    "      text.append(block[\"Text\"])\n",
    "\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "QTGZB2vmMiiQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkguwCP5fXjp"
   },
   "source": [
    "In the end, AWS Textract is giving best result, there are few minor problems here and there, but LLM can fix those.\n",
    "\n",
    "Let's extract all the text from full pdf and pass it to Llama to correct the mistakes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmSGkBKGMmEh",
    "outputId": "dfdd13bd-d73a-4e7b-94d5-ba04a0c26f42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page no 1\n",
      "Processing page no 2\n",
      "Processing page no 3\n",
      "Processing page no 4\n",
      "Processing page no 5\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "def extract_text_from_scanned_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from scanned PDF using OCR.\"\"\"\n",
    "    # Convert PDF pages to images\n",
    "    all_text = []\n",
    "    pages = convert_from_path(pdf_path)\n",
    "    for idx, page in enumerate(pages):\n",
    "      print(f\"Processing page no {idx+1}\")\n",
    "      img_bytes_io = io.BytesIO()\n",
    "      page.save(img_bytes_io, format=\"PNG\")\n",
    "      img_bytes = img_bytes_io.getvalue()\n",
    "\n",
    "      # Pass to Textract\n",
    "      response = client.detect_document_text(\n",
    "          Document={\"Bytes\": img_bytes}\n",
    "      )\n",
    "\n",
    "      # Extract text from blocks\n",
    "      text = []\n",
    "      for block in response[\"Blocks\"]:\n",
    "          if block[\"BlockType\"] == \"LINE\":\n",
    "            text.append(block[\"Text\"])\n",
    "      all_text.append(text)\n",
    "    return all_text\n",
    "\n",
    "output = extract_text_from_scanned_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TbRwzIoyRwyV",
    "outputId": "88515994-c26a-4f8f-e541-06d9ac80c870"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['DATE:',\n",
       "  'PAGE:',\n",
       "  'Chapter-1',\n",
       "  'Cinderella',\n",
       "  'I',\n",
       "  'Word wall :-',\n",
       "  'Cruel',\n",
       "  'Fairy Godmother',\n",
       "  'Messenger Warned',\n",
       "  'Invitation Glass slipper',\n",
       "  'Royal ball Cinderella',\n",
       "  '11',\n",
       "  'Word meaning:',\n",
       "  '1.',\n",
       "  'Cruel- - someone who causes pain',\n",
       "  'to others',\n",
       "  '2.',\n",
       "  'Messenger - a person who gives'],\n",
       " ['DATE:',\n",
       "  'PAGE:',\n",
       "  '1',\n",
       "  'a message to people',\n",
       "  '3.',\n",
       "  'Royal - related to the king or queen',\n",
       "  '4.',\n",
       "  'Ball - a large formal party with',\n",
       "  'dancing',\n",
       "  '5.',\n",
       "  'Meekly - quietly',\n",
       "  '6.',\n",
       "  \"Midnight - 12:0'clock at night\",\n",
       "  'III',\n",
       "  'Question and Answer',\n",
       "  'Q1.',\n",
       "  \"Why did the King's messenger come\",\n",
       "  \"at Cinderella's doorstep?\",\n",
       "  'A1.',\n",
       "  \"The King's messenger came at\",\n",
       "  \"Cinderella's doorstep with an\"],\n",
       " ['DATE:',\n",
       "  'PAGE:',\n",
       "  'invitation to the royal ball.',\n",
       "  'Q2.',\n",
       "  'Why was the King organising the',\n",
       "  'royal ball?',\n",
       "  'A2.',\n",
       "  'The King was organising the royal',\n",
       "  'ball as he wanted his son to',\n",
       "  'choose a bride.',\n",
       "  'Q3.',\n",
       "  \"Why did Cinderella's stepmother\",\n",
       "  'not want her to attend the',\n",
       "  'royal ball ?'],\n",
       " ['DATE:',\n",
       "  'PAGE:',\n",
       "  '1',\n",
       "  'A3,',\n",
       "  'The stepmother did not want',\n",
       "  'Cinderella to attend the ball as',\n",
       "  'she was jealous of her beauty.',\n",
       "  'IV',\n",
       "  'Activity Genders',\n",
       "  'Masculine LT: Feminine',\n",
       "  '1.',\n",
       "  'Son Datry Daughter',\n",
       "  '2.',\n",
       "  'Milkman M - Milkmaid',\n",
       "  '3.',\n",
       "  'Peacock - - P Peahen',\n",
       "  '4.',\n",
       "  'Fox',\n",
       "  'Vixen Vixen',\n",
       "  '5.',\n",
       "  'Gander - Goose'],\n",
       " ['DATE:',\n",
       "  'PAGE: -',\n",
       "  'V',\n",
       "  'Homophones',\n",
       "  '1.',\n",
       "  'Wait - Weight',\n",
       "  '2.',\n",
       "  'Hair - Hare',\n",
       "  '3.',\n",
       "  'Hear - Here',\n",
       "  'it',\n",
       "  'Nose - Knows',\n",
       "  '5.',\n",
       "  'Be - Bee']]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C94Cf_DDf0II"
   },
   "source": [
    "Using Llama through Groq to make the text coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "l2HSD6bvUCFU"
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "1NFWgHW_SkKS"
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "grok_key = os.getenv(\"GROK_KEY\")\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    api_key=grok_key,\n",
    "    temperature=0.1\n",
    ")\n",
    "# Prompt to instruct LLM to fix issues.\n",
    "ocr_prompt = \"\"\"\"\n",
    "You are an OCR and language expert.\n",
    "You have been provided an extracted text from students notes.\n",
    "Some text maybe divided into multiple lines.\n",
    "You job is to combine the text and give a clean output.\n",
    "Only combine text based on the order given.\n",
    "Ignore these words: [\"DATE:\",\"PAGE:\"]\n",
    "\n",
    "text: \\n\"\"\"\n",
    "extracted_text = '\\n'.join(['\\n'.join(x) for x in output])\n",
    "ocr_extracted_input = ocr_prompt + extracted_text\n",
    "\n",
    "llm_response = llm.invoke(ocr_extracted_input).content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0LXZgBROT6G-",
    "outputId": "45418228-3e01-48f4-8e1a-e1e6d03d8b47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE:\n",
      "PAGE:\n",
      "Chapter-1\n",
      "Cinderella\n",
      "I\n",
      "Word wall :-\n",
      "Cruel\n",
      "Fairy Godmother\n",
      "Messenger Warned\n",
      "Invitation Glass slipper\n",
      "Royal ball Cinderella\n",
      "11\n",
      "Word meaning:\n",
      "1.\n",
      "Cruel- - someone who causes pain\n",
      "to others\n",
      "2.\n",
      "Messenger - a person who gives\n",
      "DATE:\n",
      "PAGE:\n",
      "1\n",
      "a message to people\n",
      "3.\n",
      "Royal - related to the king or queen\n",
      "4.\n",
      "Ball - a large formal party with\n",
      "dancing\n",
      "5.\n",
      "Meekly - quietly\n",
      "6.\n",
      "Midnight - 12:0'clock at night\n",
      "III\n",
      "Question and Answer\n",
      "Q1.\n",
      "Why did the King's messenger come\n",
      "at Cinderella's doorstep?\n",
      "A1.\n",
      "The King's messenger came at\n",
      "Cinderella's doorstep with an\n",
      "DATE:\n",
      "PAGE:\n",
      "invitation to the royal ball.\n",
      "Q2.\n",
      "Why was the King organising the\n",
      "royal ball?\n",
      "A2.\n",
      "The King was organising the royal\n",
      "ball as he wanted his son to\n",
      "choose a bride.\n",
      "Q3.\n",
      "Why did Cinderella's stepmother\n",
      "not want her to attend the\n",
      "royal ball ?\n",
      "DATE:\n",
      "PAGE:\n",
      "1\n",
      "A3,\n",
      "The stepmother did not want\n",
      "Cinderella to attend the ball as\n",
      "she was jealous of her beauty.\n",
      "IV\n",
      "Activity Genders\n",
      "Masculine LT: Feminine\n",
      "1.\n",
      "Son Datry Daughter\n",
      "2.\n",
      "Milkman M - Milkmaid\n",
      "3.\n",
      "Peacock - - P Peahen\n",
      "4.\n",
      "Fox\n",
      "Vixen Vixen\n",
      "5.\n",
      "Gander - Goose\n",
      "DATE:\n",
      "PAGE: -\n",
      "V\n",
      "Homophones\n",
      "1.\n",
      "Wait - Weight\n",
      "2.\n",
      "Hair - Hare\n",
      "3.\n",
      "Hear - Here\n",
      "it\n",
      "Nose - Knows\n",
      "5.\n",
      "Be - Bee\n"
     ]
    }
   ],
   "source": [
    "#Input we passed to LLM\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XnJh7m6mUhvY",
    "outputId": "1b8b5e70-1f8b-4684-880f-109e634d74a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the cleaned and combined text based on the order given:\n",
      "\n",
      "Chapter-1\n",
      "Cinderella\n",
      "I\n",
      "Word wall :-\n",
      "Cruel - someone who causes pain to others\n",
      "Fairy Godmother\n",
      "Messenger - a person who gives a message to people\n",
      "Invitation\n",
      "Glass slipper\n",
      "Royal ball Cinderella\n",
      "11\n",
      "Word meaning:\n",
      "1. Cruel - someone who causes pain to others\n",
      "2. Messenger - a person who gives a message to people\n",
      "3. Royal - related to the king or queen\n",
      "4. Ball - a large formal party with dancing\n",
      "5. Meekly - quietly\n",
      "6. Midnight - 12:0'clock at night\n",
      "III\n",
      "Question and Answer\n",
      "Q1. Why did the King's messenger come at Cinderella's doorstep?\n",
      "A1. The King's messenger came at Cinderella's doorstep with an invitation to the royal ball.\n",
      "Q2. Why was the King organising the royal ball?\n",
      "A2. The King was organising the royal ball as he wanted his son to choose a bride.\n",
      "Q3. Why did Cinderella's stepmother not want her to attend the royal ball?\n",
      "A3. The stepmother did not want Cinderella to attend the ball as she was jealous of her beauty.\n",
      "IV\n",
      "Activity Genders\n",
      "Masculine LT: Feminine\n",
      "1. Son - Daughter\n",
      "2. Milkman - Milkmaid\n",
      "3. Peacock - Peahen\n",
      "4. Fox - Vixen\n",
      "5. Gander - Goose\n",
      "V\n",
      "Homophones\n",
      "1. Wait - Weight\n",
      "2. Hair - Hare\n",
      "3. Hear - Here\n",
      "4. Nose - Knows\n",
      "5. Be - Bee\n"
     ]
    }
   ],
   "source": [
    "# Output from LLM\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzD18IMUgIBw"
   },
   "source": [
    "So, with AWS Textract and LLM, we are able to read whole text quite well.\n",
    "\n",
    "I could have used AWS Bedrock for LLM as well, but chose groq since it's free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBVA4p_WgZaY"
   },
   "source": [
    "Now Let's extract questions and answers from the whole text in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "A9TGhaJVUm-E"
   },
   "outputs": [],
   "source": [
    "extract_question_answer_prompt = \"\"\"\"\n",
    "You are a language expert.\n",
    "You are given a list of questions and answers.\n",
    "Do not return any extra information.\n",
    "Return the question-answer pair in list of dictionary format as mentioned below:\n",
    "[{\"question\":..., \"answer\":...},\n",
    "{\"question\":..., \"answer\":...},\n",
    "]\"\"\"\n",
    "\n",
    "extract_question_answer_with_data = extract_question_answer_prompt + llm_response\n",
    "\n",
    "question_answer = llm.invoke(extract_question_answer_with_data).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glwV6PuWXEgB",
    "outputId": "39ddc8a3-b3ca-41bb-eff1-98941408d10d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': \"Why did the King's messenger come at Cinderella's doorstep?\",\n",
       "  'answer': \"The King's messenger came at Cinderella's doorstep with an invitation to the royal ball.\"},\n",
       " {'question': 'Why was the King organising the royal ball?',\n",
       "  'answer': 'The King was organising the royal ball as he wanted his son to choose a bride.'},\n",
       " {'question': \"Why did Cinderella's stepmother not want her to attend the royal ball?\",\n",
       "  'answer': 'The stepmother did not want Cinderella to attend the ball as she was jealous of her beauty.'}]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(question_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaXqkid1grVz"
   },
   "source": [
    "**This is pretty good.**\n",
    "\n",
    "**We can use this and more pdfs later in our RAG application later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foNgC8E3gxMO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0496c7078b3a44978b5ffffb5801a5fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2eea0fdb37214d6bab9fd7fb01ecc42c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65a541da27ba477482606b3319bce285",
      "placeholder": "​",
      "style": "IPY_MODEL_e7698ef59bf04e42baa881c63121432d",
      "value": " 1/1 [00:00&lt;00:00,  9.43it/s]"
     }
    },
    "65a541da27ba477482606b3319bce285": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66e5dfa5cef240e5b013883a831d8c39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aa0beb1c65ce4e17a983b5e59bbeec6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8b4045611354038a3245658bff81f34",
      "placeholder": "​",
      "style": "IPY_MODEL_e40d834c1f1d49988d8cf7dc50625a1e",
      "value": "Fetching 1 files: 100%"
     }
    },
    "ad4f2bf792674683bb929c7e82b2a49b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0496c7078b3a44978b5ffffb5801a5fa",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_66e5dfa5cef240e5b013883a831d8c39",
      "value": 1
     }
    },
    "cdcd88effa124a5e9c40c25f8dd336df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa0beb1c65ce4e17a983b5e59bbeec6e",
       "IPY_MODEL_ad4f2bf792674683bb929c7e82b2a49b",
       "IPY_MODEL_2eea0fdb37214d6bab9fd7fb01ecc42c"
      ],
      "layout": "IPY_MODEL_e54bad369d04446b808a8d0ae250796e"
     }
    },
    "d8b4045611354038a3245658bff81f34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e40d834c1f1d49988d8cf7dc50625a1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e54bad369d04446b808a8d0ae250796e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7698ef59bf04e42baa881c63121432d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
